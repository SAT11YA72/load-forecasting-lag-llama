{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "11bde39e-dfcb-47df-b15f-4abe83cf9431",
      "metadata": {
        "id": "11bde39e-dfcb-47df-b15f-4abe83cf9431"
      },
      "source": [
        "# Lag-Llama Fine Tuning for Short Term Load Forecasting\n",
        "\n",
        "## Prepare the repository\n",
        "\n",
        "We first clone and install the required packages from the [GitHub repository](https://github.com/time-series-foundation-models/lag-llama/) that has the Lag-Llama architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f314d613-aac8-499f-a632-3a9062e1293e",
      "metadata": {
        "id": "f314d613-aac8-499f-a632-3a9062e1293e"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/time-series-foundation-models/lag-llama/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b3a4589-1ff7-46b3-8ad3-b2c2bc998183",
      "metadata": {
        "id": "9b3a4589-1ff7-46b3-8ad3-b2c2bc998183"
      },
      "outputs": [],
      "source": [
        "cd /content/lag-llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb4157e-a745-4938-b8c8-06eeeb57eeaa",
      "metadata": {
        "id": "1cb4157e-a745-4938-b8c8-06eeeb57eeaa"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2zs1qkcUCObG",
      "metadata": {
        "id": "2zs1qkcUCObG"
      },
      "source": [
        "We are downloading the pretrained model weights from hugging face here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N4_K8Qv6_ZSk",
      "metadata": {
        "id": "N4_K8Qv6_ZSk"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir /content/lag-llama"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c2ea0b9-4b10-4a8a-a973-f8a53c4993e3",
      "metadata": {
        "id": "7c2ea0b9-4b10-4a8a-a973-f8a53c4993e3"
      },
      "source": [
        "## Imports\n",
        "\n",
        "We import the required packages and the lag llama estimator object which we can use to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12626ee4-7175-4854-8701-7cc6a4f514d8",
      "metadata": {
        "id": "12626ee4-7175-4854-8701-7cc6a4f514d8"
      },
      "outputs": [],
      "source": [
        "from itertools import islice\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import torch\n",
        "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
        "from gluonts.dataset.repository.datasets import get_dataset\n",
        "\n",
        "from gluonts.dataset.pandas import PandasDataset\n",
        "import pandas as pd\n",
        "\n",
        "from lag_llama.gluon.estimator import LagLlamaEstimator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19313809-514c-4eca-8bd2-7de76393bda5",
      "metadata": {
        "id": "19313809-514c-4eca-8bd2-7de76393bda5"
      },
      "source": [
        "We create a function for Lag-Llama inference that we can reuse. This function returns the predictions for the given prediction horizon. The forecast will be of shape (num_samples, prediction_length), where `num_samples` is the number of samples sampled from the predicted probability distribution for each timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c2405a-be1c-4f7d-b598-580c974a68fb",
      "metadata": {
        "id": "e4c2405a-be1c-4f7d-b598-580c974a68fb"
      },
      "outputs": [],
      "source": [
        "def get_lag_llama_predictions(dataset, prediction_length, context_length=32, num_samples=20, device=\"cuda\", batch_size=64, nonnegative_pred_samples=True):\n",
        "    ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
        "    estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
        "\n",
        "    estimator = LagLlamaEstimator(\n",
        "        ckpt_path=\"lag-llama.ckpt\",\n",
        "        prediction_length=prediction_length,\n",
        "        context_length=context_length,\n",
        "\n",
        "        # estimator args\n",
        "        input_size=estimator_args[\"input_size\"],\n",
        "        n_layer=estimator_args[\"n_layer\"],\n",
        "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
        "        n_head=estimator_args[\"n_head\"],\n",
        "        scaling=estimator_args[\"scaling\"],\n",
        "        time_feat=estimator_args[\"time_feat\"],\n",
        "\n",
        "        nonnegative_pred_samples=nonnegative_pred_samples,\n",
        "\n",
        "        # linear positional encoding scaling\n",
        "        rope_scaling={\n",
        "            \"type\": \"linear\",\n",
        "            \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
        "        },\n",
        "\n",
        "        batch_size=batch_size,\n",
        "        num_parallel_samples=num_samples,\n",
        "    )\n",
        "\n",
        "    lightning_module = estimator.create_lightning_module()\n",
        "    transformation = estimator.create_transformation()\n",
        "    predictor = estimator.create_predictor(transformation, lightning_module)\n",
        "\n",
        "    forecast_it, ts_it = make_evaluation_predictions(\n",
        "        dataset=dataset,\n",
        "        predictor=predictor,\n",
        "        num_samples=num_samples\n",
        "    )\n",
        "    forecasts = list(tqdm(forecast_it, total=len(dataset), desc=\"Forecasting batches\"))\n",
        "    tss = list(tqdm(ts_it, total=len(dataset), desc=\"Ground truth\"))\n",
        "\n",
        "    return forecasts, tss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b5531e-8ad3-4465-b400-33465eff0340",
      "metadata": {
        "id": "06b5531e-8ad3-4465-b400-33465eff0340"
      },
      "source": [
        "## Zero Shot Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KgzgE2E7VjSC",
      "metadata": {
        "id": "KgzgE2E7VjSC"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"/content/train_1HOUR.csv\",parse_dates=True)\n",
        "df_test = pd.read_csv(\"/content/test_1HOUR.csv\",parse_dates=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b687e442-26b4-443e-b74a-19e8810d0836",
      "metadata": {
        "id": "b687e442-26b4-443e-b74a-19e8810d0836"
      },
      "outputs": [],
      "source": [
        "prediction_length = 6\n",
        "context_length = prediction_length*5\n",
        "num_samples = 100\n",
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D8VsI6utF8-x",
      "metadata": {
        "id": "D8VsI6utF8-x"
      },
      "outputs": [],
      "source": [
        "# Set numerical columns as float32\n",
        "for col in df_train.columns:\n",
        "    # Check if column is not of string type\n",
        "    if df_train[col].dtype != 'object' and pd.api.types.is_string_dtype(df_train[col]) == False:\n",
        "        df_train[col] = df_train[col].astype('float32')\n",
        "\n",
        "for col in df_test.columns:\n",
        "    # Check if column is not of string type\n",
        "    if df_test[col].dtype != 'object' and pd.api.types.is_string_dtype(df_test[col]) == False:\n",
        "        df_test[col] = df_test[col].astype('float32')\n",
        "\n",
        "# Create the Pandas\n",
        "ds_train = PandasDataset.from_long_dataframe(df_train, target=\"t_kW\", item_id=\"item_id\")\n",
        "ds_test = PandasDataset.from_long_dataframe(df_test, target=\"t_kW\", item_id=\"item_id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf91cf2-6257-4202-83de-94e0c63751fa",
      "metadata": {
        "id": "9cf91cf2-6257-4202-83de-94e0c63751fa"
      },
      "outputs": [],
      "source": [
        "forecasts, tss = get_lag_llama_predictions(\n",
        "    ds_test,\n",
        "    prediction_length=prediction_length,\n",
        "    num_samples=num_samples,\n",
        "    context_length=context_length,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "InRf-aCJXPAp",
      "metadata": {
        "id": "InRf-aCJXPAp"
      },
      "outputs": [],
      "source": [
        "forecasts = list(forecasts)\n",
        "tss = list(tss)\n",
        "\n",
        "evaluator = Evaluator()\n",
        "agg_metrics, ts_metrics = evaluator(iter(tss), iter(forecasts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lJBpgNsgGq5U",
      "metadata": {
        "id": "lJBpgNsgGq5U"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 15))\n",
        "date_formater = mdates.DateFormatter('%b, %d')\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "# Iterate through the first 9 series, and plot the predicted samples\n",
        "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 2):\n",
        "    ax = plt.subplot(3, 3, idx+1)\n",
        "\n",
        "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
        "    forecast.plot( color='g')\n",
        "    plt.xticks(rotation=60)\n",
        "    ax.xaxis.set_major_formatter(date_formater)\n",
        "    ax.set_title(forecast.item_id)\n",
        "\n",
        "plt.gcf().tight_layout()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aq5L97jXWeO9",
      "metadata": {
        "id": "aq5L97jXWeO9"
      },
      "outputs": [],
      "source": [
        "print(\"MSE:\", agg_metrics['MSE'])\n",
        "print(\"CRPS:\", agg_metrics['mean_wQuantileLoss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7a9f417-784e-4425-bccf-7fdb10a0df56",
      "metadata": {
        "id": "a7a9f417-784e-4425-bccf-7fdb10a0df56"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Let us fine-tune the Lag-Llama base model with a few data-specific changes. Feel Free to change the hyperparameters below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de228599-5404-4b62-87cd-4b9573583981",
      "metadata": {
        "id": "de228599-5404-4b62-87cd-4b9573583981"
      },
      "outputs": [],
      "source": [
        "ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
        "estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
        "\n",
        "estimator = LagLlamaEstimator(\n",
        "        ckpt_path=\"lag-llama.ckpt\",\n",
        "        prediction_length=prediction_length,\n",
        "        context_length=context_length,\n",
        "\n",
        "        # distr_output=\"neg_bin\",\n",
        "        # scaling=\"mean\",\n",
        "        nonnegative_pred_samples=True,\n",
        "        aug_prob=0,\n",
        "        lr=5e-4,\n",
        "\n",
        "        # estimator args\n",
        "        input_size=estimator_args[\"input_size\"],\n",
        "        n_layer=estimator_args[\"n_layer\"],\n",
        "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
        "        n_head=estimator_args[\"n_head\"],\n",
        "        time_feat=estimator_args[\"time_feat\"],\n",
        "\n",
        "        # rope_scaling={\n",
        "        #     \"type\": \"linear\",\n",
        "        #     \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
        "        # },\n",
        "\n",
        "        batch_size=64,\n",
        "        num_parallel_samples=num_samples,\n",
        "        trainer_kwargs = {\"max_epochs\": 50,}, # <- lightning trainer arguments\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b2b75bd-657d-438b-8596-f97509253c92",
      "metadata": {
        "id": "7b2b75bd-657d-438b-8596-f97509253c92"
      },
      "outputs": [],
      "source": [
        "predictor = estimator.train(ds_train, cache_data=True, shuffle_buffer_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a88620-5a56-463c-adf7-ef13018da432",
      "metadata": {
        "id": "54a88620-5a56-463c-adf7-ef13018da432"
      },
      "outputs": [],
      "source": [
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "        dataset=ds_test,\n",
        "        predictor=predictor,\n",
        "        num_samples=num_samples\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3413cbf1-7e58-4c16-84a1-4c92d9926908",
      "metadata": {
        "id": "3413cbf1-7e58-4c16-84a1-4c92d9926908"
      },
      "outputs": [],
      "source": [
        "forecasts = list(tqdm(forecast_it, total=len(ds_test), desc=\"Forecasting batches\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf547aa-4c47-40b5-bf14-0835601a24d7",
      "metadata": {
        "id": "acf547aa-4c47-40b5-bf14-0835601a24d7"
      },
      "outputs": [],
      "source": [
        "tss = list(tqdm(ts_it, total=len(ds_test), desc=\"Ground truth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a310d3c-961c-412f-983d-039f5e04fbf7",
      "metadata": {
        "id": "6a310d3c-961c-412f-983d-039f5e04fbf7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 15))\n",
        "date_formater = mdates.DateFormatter('%b, %d')\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "# Iterate through the first 9 series, and plot the predicted samples\n",
        "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 2):\n",
        "    ax = plt.subplot(3, 3, idx+1)\n",
        "\n",
        "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
        "    forecast.plot( color='g')\n",
        "    plt.xticks(rotation=60)\n",
        "    ax.xaxis.set_major_formatter(date_formater)\n",
        "    ax.set_title(forecast.item_id)\n",
        "\n",
        "plt.gcf().tight_layout()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9360f22e-7438-4f5b-a20b-6ab83e25d3c4",
      "metadata": {
        "id": "9360f22e-7438-4f5b-a20b-6ab83e25d3c4"
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator()\n",
        "agg_metrics, ts_metrics = evaluator(iter(tss), iter(forecasts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g8oII2IbcPQw",
      "metadata": {
        "id": "g8oII2IbcPQw"
      },
      "outputs": [],
      "source": [
        "print(\"MSE:\", agg_metrics['MSE'])\n",
        "print(\"CRPS:\", agg_metrics['mean_wQuantileLoss'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}